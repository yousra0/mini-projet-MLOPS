from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
import joblib
import numpy as np
import mlflow
from mlflow.tracking import MlflowClient
import logging
from logger_config import send_log, logger  # Importation du logger et de la fonction d'envoi des logs

# ğŸ“Œ Ã‰tape 1 : Configuration de MLflow et du client de suivi
print("\n" + "â€”" * 80)
print("ğŸ”§ CONFIGURATION DE MLFLOW")
mlflow.set_tracking_uri("http://127.0.0.1:5000")
client = MlflowClient()

# ğŸ” VÃ©rifier la connexion Ã  MLflow
try:
    experiments = client.search_experiments()
    print("âœ… Connexion rÃ©ussie Ã  MLflow ! ExpÃ©riences disponibles :", [exp.name for exp in experiments])
except Exception as e:
    print(f"ğŸš¨ Erreur de connexion Ã  MLflow : {e}")

EXPERIMENT_NAME = "Churn Prediction Experiment"

# ğŸ“Œ Ã‰tape 2 : VÃ©rification de l'existence de l'expÃ©rience MLflow
experiment = client.get_experiment_by_name(EXPERIMENT_NAME)
if experiment is None:
    experiment_id = client.create_experiment(EXPERIMENT_NAME)
    print(f"âœ… Nouvelle expÃ©rience MLflow crÃ©Ã©e : ID {experiment_id}")
else:
    experiment_id = experiment.experiment_id
    print(f"ğŸ”„ ExpÃ©rience MLflow existante : ID {experiment_id}")

mlflow.set_experiment(EXPERIMENT_NAME)
print("â€”" * 80 + "\n")

# ğŸ“Œ Ã‰tape 3 : Chargement du modÃ¨le
MODEL_PATH = "random_forest_model.joblib"
try:
    model = joblib.load(MODEL_PATH)
    logger.info("âœ… ModÃ¨le Random Forest chargÃ© avec succÃ¨s")
except Exception as e:
    logger.error(f"ğŸš¨ Erreur lors du chargement du modÃ¨le: {e}")
    raise RuntimeError(f"ğŸš¨ Erreur lors du chargement du modÃ¨le: {e}")

# ğŸ“Œ Ã‰tape 4 : Initialisation de l'application FastAPI
app = FastAPI()

# ğŸ“Œ DÃ©finition du format d'entrÃ©e pour les requÃªtes de prÃ©diction
class PredictionInput(BaseModel):
    features: list

# ğŸ“Œ Ã‰tape 5 : Endpoints de l'API

## ğŸ¥ VÃ©rifier que le service fonctionne bien
@app.get("/health")
def health_check():
    return {
        "status": "ok",
        "model_loaded": model is not None
    }

## ğŸ‘‹ Endpoint d'accueil
@app.get("/")
def hello_world():
    return {"message": "ğŸ‘‹ Hello, Yousra! ğŸš€"}

## ğŸ”® Endpoint de prÃ©diction
@app.post("/predict")
def predict(data: PredictionInput):
    try:
        # VÃ©rifier que les features sont bien une liste non vide
        if not isinstance(data.features, list) or len(data.features) == 0:
            raise ValueError("âŒ Les features doivent Ãªtre une liste non vide.")

        # Transformer l'entrÃ©e en tableau numpy et faire la prÃ©diction
        X_input = np.array(data.features).reshape(1, -1)
        prediction = model.predict(X_input)
        prediction_value = int(prediction[0])

        # ğŸ” Enregistrer la prÃ©diction avec MLflow
        with mlflow.start_run(experiment_id=experiment_id):
            mlflow.log_metric("prediction", prediction_value)

        # ğŸ“¡ Envoyer les logs Ã  Elasticsearch
        send_log("mlflow-metrics", {
            "prediction": prediction_value,
            "features": data.features
        }, experiment=EXPERIMENT_NAME)

        return {"prediction": prediction_value}

    except ValueError as ve:
        logger.error(f"âš ï¸ Erreur de validation des donnÃ©es: {ve}")
        raise HTTPException(status_code=400, detail=f"âš ï¸ Erreur: {ve}")

    except Exception as e:
        logger.error(f"ğŸš¨ Erreur lors de la prÃ©diction: {e}")
        raise HTTPException(status_code=500, detail=f"ğŸš¨ Erreur lors de la prÃ©diction: {e}")

# ğŸ“Œ Ã‰tape 6 : Point d'entrÃ©e pour exÃ©cuter l'application avec Uvicorn
if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)
